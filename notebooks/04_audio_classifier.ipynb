{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio classificiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs audio classification in pytorch, using the [SpeechCommands dataset](https://arxiv.org/abs/1804.03209).\n",
    "\n",
    "This code implements a 1-Dimensional Convoluitonal Neural Network that classifies raw waveforms of people speaking different voice instructions. The CNN model implemented here is based on [this paper](https://arxiv.org/pdf/1610.00087.pdf).\n",
    "\n",
    "The code in this notebook is heavily modified (for readability and adaptability) from [this source](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/c64f4bad00653411821adcb75aea9015/speech_command_classification_with_torchaudio_tutorial.ipynb), adapted by [Terence Broad](https://researchers.arts.ac.uk/2351-terence-broad) and [Irini Kalaitzidi](https://www.gold.ac.uk/computing/people/kalaitzidi-irini/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchaudio\n",
    "# 'Audio Transforms'\n",
    "import torchaudio.transforms as AT\n",
    "\n",
    "# Get cpu, gpu or mps device for training\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    !pip install torchcodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed directory structure --------------\n",
    "DATASETS_DIR = pathlib.Path(\"datasets\")\n",
    "DATASETS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODELS_DIR = pathlib.Path(\"models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "# ----------------------------------------\n",
    "\n",
    "# change accordingly\n",
    "MODEL_NAME = \"speechcommands_conv\"\n",
    "\n",
    "MODEL_DIR = MODELS_DIR / \"speechcommands\"\n",
    "MODEL_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE_ORIG = 16000 # Sample rate for the speech commands dataset\n",
    "SAMPLE_RATE_NEW = 8000   # Sample rate for our model\n",
    "BATCH_SIZE = 200         # Batch size for training\n",
    "LEARNING_RATE = 0.01     # Learning rate for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with torchaudio we can only define one transform here. This function downsamples the audio waveform from a sample rate of 16000 to 8000, which is fine for working with human voices and helps us train more efficiently.\n",
    "\n",
    "Unlike when working with images, the padding and normalising of the data to the same length happens in the function `collate_audio_folder_batch`. As all of our audio files are different lengths, we need to harmonise them when we load in a random mini-batch. \n",
    "\n",
    "Note: [Resampling tutorial](https://docs.pytorch.org/audio/stable/tutorials/audio_resampling_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = AT.Resample(\n",
    "    orig_freq=SAMPLE_RATE_ORIG,\n",
    "    new_freq=SAMPLE_RATE_NEW\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_classes(dirname):\n",
    "    classes = [d for d in os.listdir(dirname) if os.path.isdir(os.path.join(dirname, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "\n",
    "class SpeechCommandsDataset(torchaudio.datasets.SPEECHCOMMANDS):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root = DATASETS_DIR,\n",
    "        url = 'speech_commands_v0.02',\n",
    "        folder_in_archive = \"SpeechCommands\",\n",
    "        download = True,\n",
    "        subset = None,\n",
    "        transform = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            root = root,\n",
    "            url = url,\n",
    "            folder_in_archive = folder_in_archive,\n",
    "            download = download,\n",
    "            subset = subset\n",
    "        )\n",
    "\n",
    "        classes, class_to_idx = find_classes(os.path.join(root, folder_in_archive, url))\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = class_to_idx \n",
    "        self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # https://github.com/pytorch/audio/blob/e1232690308a6b5297fcd06e925899a9b64f7280/src/torchaudio/datasets/speechcommands.py#L158\n",
    "        # metadata is (path, sample rate, label (str), speaker ID, utterance number)\n",
    "        metadata = self.get_metadata(index)\n",
    "\n",
    "        waveform = torchaudio.datasets.utils._load_waveform(self._archive, metadata[0], metadata[1])\n",
    "        if self.transform is not None:\n",
    "            waveform = self.transform(waveform)\n",
    "        \n",
    "        # return only (waveform, class index)\n",
    "        return (waveform, torch.tensor(self.class_to_idx[metadata[2]]))\n",
    "\n",
    "train_dataset = SpeechCommandsDataset(\n",
    "    subset = \"training\",\n",
    "    transform = transform\n",
    ")\n",
    "val_dataset = SpeechCommandsDataset(\n",
    "    subset = \"validation\",\n",
    "    transform = transform\n",
    ")\n",
    "test_dataset = SpeechCommandsDataset(\n",
    "    subset = \"testing\",\n",
    "    transform = transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.get_metadata(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    # print(batch[0].shape)\n",
    "    # batch has samples of shape (channels, timesteps) (usually: 1, ts), we need to pass them as (ts, C)\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    # batch is now (B, TS, C), permuting back to (B, C, TS)\n",
    "    # print(batch.shape)\n",
    "    # Einstein summation notation: https://docs.pytorch.org/docs/stable/generated/torch.einsum.html\n",
    "    return torch.einsum(\"btc -> bct\", batch)\n",
    "\n",
    "def collate_audio_folder_batch(batch):\n",
    "    # Empty lists for adding data too\n",
    "    tensors, targets = [], []\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, label in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label]\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    targets = torch.stack(targets)\n",
    "    return tensors, targets\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_audio_folder_batch,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,    \n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_audio_folder_batch,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,    \n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_audio_folder_batch,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "for i, (waveform, label) in enumerate(train_loader):\n",
    "    print(waveform.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch, label_batch = next(iter(train_loader))\n",
    "sample_waveform = data_batch[0].squeeze()\n",
    "sample_class = label_batch[0].item()\n",
    "\n",
    "print(f\"Data batch shape: {data_batch.shape} (B, C, TS)\")\n",
    "print(f\"Shape of waveform: {sample_waveform.size()}\")\n",
    "print(f\"Class of waveform: '{train_dataset.idx_to_class[sample_class]}'\")\n",
    "\n",
    "plt.plot(sample_waveform.t().numpy())\n",
    "ipd.Audio(sample_waveform.numpy(), rate=SAMPLE_RATE_NEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a 1-Dimensional convolutional neural network to process raw audio data. The specific architecture is modeled after the M5 network architecture described in [this paper](https://arxiv.org/pdf/1610.00087.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=NUM_CLASSES, stride=16, n_channel=32, kernel_size=80):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup core objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we setup our core objects, the model, the loss function (criterion) and the optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = M5(n_input=1, n_output=NUM_CLASSES)\n",
    "model = M5()\n",
    "print(model)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "n = count_parameters(model)\n",
    "print(f\"Number of parameters: {n}.\")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=0.0001\n",
    ")\n",
    "\n",
    "# reduce the learning after 20 epochs by a factor of 10\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=20,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_correct(preds, labels):\n",
    "    # count number of correct predictions\n",
    "    return preds.squeeze().eq(labels).sum().item()\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our training loop for our data. Look at how the training set and validation set are used differently. \n",
    "\n",
    "What differences are there in the code when we cycle through each of these sets of data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs for training (this is a large dataset so not many epochs needed)\n",
    "NUM_EPOCHS = 5 \n",
    "\n",
    "# Log process every n interations\n",
    "PRINT_EVERY = 100\n",
    "\n",
    "# initialising loss lists here, so that we can do multiple runs\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_loss = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model on device\n",
    "model.to(device)\n",
    "\n",
    "# For each cycle of the dataset\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    # Variables to keep track of running loss\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Train loop (this could be wrapped into a function)\n",
    "\n",
    "    # Put model on device & in training mode\n",
    "    model.train()\n",
    "\n",
    "    t = time.time()\n",
    "    \n",
    "    # For each batch in one cycle of the training set\n",
    "    for batch_idx, (batch, labels) in enumerate(train_loader):\n",
    "\n",
    "        # Move batch to whatever device we are running training on\n",
    "        batch = batch.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass with the model\n",
    "        output = model(batch)\n",
    "\n",
    "        # Evaluate classification accuracy\n",
    "        loss = criterion(output.squeeze(), labels)\n",
    "        \n",
    "        # Backpropagate loss and update gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Keep track off loss over time\n",
    "        train_loss += loss.item()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # print training stats\n",
    "        if batch_idx % PRINT_EVERY == 0 or batch_idx == len(train_loader) - 1:\n",
    "            len_fmt = len(str(len(train_loader)))\n",
    "            msg = f\"Train Epoch: {epoch + 1} [{batch_idx:>{len_fmt}}/{len(train_loader)} \"\n",
    "            msg += f\"({100. * batch_idx / len(train_loader):>3.0f}%)] | Loss: {loss.item():.6f}\"\n",
    "            msg += f\" | {time.time() - t:.2f}s\"\n",
    "            print(msg)\n",
    "            t = time.time()\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Validation loop (could also be wrapped into a function)\n",
    "\n",
    "    # Put model in evaluation mode (turn off batch norm)\n",
    "    model.eval()\n",
    "\n",
    "    # Without gradient tracking \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Variable to track total correct classifications\n",
    "        correct = 0\n",
    "\n",
    "        # Validation loop\n",
    "        # For each batch in one cycle of the validation set\n",
    "        for batch, targets in val_loader:\n",
    "            \n",
    "            # Move batch to whatever device we are running training on\n",
    "            batch = batch.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass with the model\n",
    "            output = model(batch)\n",
    "\n",
    "            # Evaluate classification accuracy\n",
    "            loss = criterion(output.squeeze(), targets)\n",
    "            \n",
    "            # Track loss\n",
    "            val_loss += loss.item()\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "            # Get top prediction\n",
    "            pred = get_likely_index(output)\n",
    "            \n",
    "            # Check if prediction is correct\n",
    "            correct += number_of_correct(pred, targets)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # Printing & stats\n",
    "    \n",
    "    # Normalise cumulative losses to dataset size\n",
    "    # train_loss = train_loss / len(train_loader)\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # # Added cumulative losses to lists for later display\n",
    "    # train_losses.append(train_loss)\n",
    "    # val_losses.append(val_loss)\n",
    "\n",
    "    msg = f\"   Val Acc: {100. * correct / len(val_loader.dataset):.0f}% ({correct}/{len(val_loader.dataset)})\"\n",
    "    msg += f\" | Val Loss: \\033[1m{val_loss:.6f}\\033[0m\"\n",
    "    print(msg)\n",
    "\n",
    "    # update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # if validation score is lowest so far, save the model\n",
    "    if val_loss < best_loss:\n",
    "        print(\"   Val loss improved, saving model.\")\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), MODEL_DIR / f\"{MODEL_NAME}.pt\")\n",
    "        torch.jit.save(torch.jit.script(model), MODEL_DIR / f\"{MODEL_NAME}_scripted.pt\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6), sharex=False)\n",
    "\n",
    "ax1.plot(train_losses)\n",
    "ax1.set_title(\"Training loss\")\n",
    "ax1.set_ylabel(\"loss\")\n",
    "\n",
    "ax2.plot(val_losses, color=\"tab:orange\")\n",
    "ax2.set_title(\"Validation loss\")\n",
    "ax2.set_xlabel(\"steps\")\n",
    "ax2.set_ylabel(\"loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load(MODEL_DIR / f\"{MODEL_NAME}_scripted.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tensor):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    tensor = tensor.to(device)\n",
    "    with torch.no_grad():    \n",
    "        # Use the model to predict the label of the waveform\n",
    "        tensor = F.pad(tensor, pad = (tensor.shape[0], SAMPLE_RATE_NEW - tensor.shape[-1] - 1,), value = 0.)\n",
    "        tensor = model(tensor[None, ...])\n",
    "        label = get_likely_index(tensor).cpu().item()\n",
    "        class_name = train_dataset.idx_to_class[label]\n",
    "    return class_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a sample from the test set (using `train_dataset`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, label = test_dataset[torch.randint(0,len(test_dataset), (1,))]\n",
    "ipd.display(ipd.Audio(waveform.numpy(), rate=SAMPLE_RATE_NEW))\n",
    "print(f\"Expected: {train_dataset.idx_to_class[label.item()]}. Predicted: {predict(waveform)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a sample from the test set (using `test_loader`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_samples, test_targets = next(iter(test_loader))\n",
    "test_sample = test_samples[0]\n",
    "test_target = test_targets[0]\n",
    "\n",
    "print(f\"Expected: {test_dataset.idx_to_class[test_target.item()]}. Predicted: {predict(test_sample)}.\")\n",
    "ipd.Audio(test_sample.numpy(), rate=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    test_correct = 0.0\n",
    "    \n",
    "    for i, (batch, targets) in enumerate(test_loader):\n",
    "        \n",
    "        # Move batch to whatever device we are running training on\n",
    "        batch = batch.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass with the model\n",
    "        output = model(batch)\n",
    "\n",
    "        # # Evaluate classification accuracy\n",
    "        # loss = criterion(output.squeeze(), targets)\n",
    "        \n",
    "        # # Track loss\n",
    "        # val_loss += loss.item()\n",
    "\n",
    "        # Get top prediction\n",
    "        pred = get_likely_index(output)\n",
    "        \n",
    "        # Check if prediction is correct\n",
    "        test_correct += number_of_correct(pred, targets)\n",
    "\n",
    "print(f\"{test_correct:.0f}/{len(test_loader.dataset)} correct samples.\")\n",
    "print(f\"Accuracy: {test_correct / len(test_loader.dataset) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check wrong classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_misclassified():\n",
    "    model.eval()\n",
    "    for i, (waveform, label) in enumerate(test_dataset):\n",
    "        utterance = test_dataset.idx_to_class[label.item()]\n",
    "        # print(waveform.shape)\n",
    "        with torch.no_grad():\n",
    "            output = predict(waveform)\n",
    "            if output != utterance:\n",
    "                print(f\"Data point #{i}. Expected: {utterance}. Predicted: {output}.\")\n",
    "                yield ipd.Audio(waveform.numpy(), rate=SAMPLE_RATE_NEW)\n",
    "    else:\n",
    "        print(\"All examples in this dataset were correctly classified!\")\n",
    "        print(\"In this case, let's just look at the last data point\")\n",
    "        print(f\"Data point #{i}. Expected: {utterance}. Predicted: {output}.\")\n",
    "        return pd.Audio(waveform.numpy(), rate=SAMPLE_RATE_NEW)\n",
    "\n",
    "browse_iter = iter(browse_misclassified())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(browse_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your own sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_wav, test_sample_rate = torchaudio.load(\"sounds/tree.wav\")\n",
    "print(test_wav.shape, test_sample_rate)\n",
    "\n",
    "ipd.Audio(test_wav[:1].numpy(), rate=test_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampler = AT.Resample(\n",
    "    test_sample_rate,\n",
    "    SAMPLE_RATE_NEW,\n",
    "    dtype=test_wav.dtype\n",
    ")\n",
    "test_sample = resampler(test_wav)[:1, 1000:9000]\n",
    "print(test_sample.shape)\n",
    "\n",
    "ipd.Audio(test_sample.numpy(), rate=SAMPLE_RATE_NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Expected: tree. Predicted: {predict(test_sample[:1])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record your own sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record(seconds=1):\n",
    "    # Colab\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import output as colab_output\n",
    "        from base64 import b64decode\n",
    "        from io import BytesIO\n",
    "        from pydub import AudioSegment\n",
    "    \n",
    "        RECORD = (\n",
    "            b\"const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\\n\"\n",
    "            b\"const b2text = blob => new Promise(resolve => {\\n\"\n",
    "            b\"  const reader = new FileReader()\\n\"\n",
    "            b\"  reader.onloadend = e => resolve(e.srcElement.result)\\n\"\n",
    "            b\"  reader.readAsDataURL(blob)\\n\"\n",
    "            b\"})\\n\"\n",
    "            b\"var record = time => new Promise(async resolve => {\\n\"\n",
    "            b\"  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\\n\"\n",
    "            b\"  recorder = new MediaRecorder(stream)\\n\"\n",
    "            b\"  chunks = []\\n\"\n",
    "            b\"  recorder.ondataavailable = e => chunks.push(e.data)\\n\"\n",
    "            b\"  recorder.start()\\n\"\n",
    "            b\"  await sleep(time)\\n\"\n",
    "            b\"  recorder.onstop = async ()=>{\\n\"\n",
    "            b\"    blob = new Blob(chunks)\\n\"\n",
    "            b\"    text = await b2text(blob)\\n\"\n",
    "            b\"    resolve(text)\\n\"\n",
    "            b\"  }\\n\"\n",
    "            b\"  recorder.stop()\\n\"\n",
    "            b\"})\"\n",
    "        )\n",
    "        RECORD = RECORD.decode(\"ascii\")\n",
    "    \n",
    "        print(f\"Recording started for {seconds} seconds.\")\n",
    "        display(ipd.Javascript(RECORD))\n",
    "        s = colab_output.eval_js(\"record(%d)\" % (seconds * 1000))\n",
    "        print(\"Recording ended.\")\n",
    "        b = b64decode(s.split(\",\")[1])\n",
    "    \n",
    "        fileformat = \"wav\"\n",
    "        filename = f\"_audio.{fileformat}\"\n",
    "        AudioSegment.from_file(BytesIO(b)).export(filename, format=fileformat)\n",
    "        return torchaudio.load(filename)\n",
    "\n",
    "    # Locally\n",
    "    else:    \n",
    "        import sounddevice as sd\n",
    "    \n",
    "        print(f\"Recording started for {seconds} seconds.\")\n",
    "        audio = sd.rec(\n",
    "            int(seconds * SAMPLE_RATE_NEW),\n",
    "            samplerate=SAMPLE_RATE_NEW,\n",
    "            channels=1,\n",
    "            dtype=\"float32\"\n",
    "        )\n",
    "        sd.wait()\n",
    "        print(\"Recording ended.\")\n",
    "    \n",
    "        waveform = torch.from_numpy(audio.T)\n",
    "        return waveform, SAMPLE_RATE_NEW    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "waveform, sample_rate = record()\n",
    "print(f\"Predicted: {predict(waveform)}.\")\n",
    "ipd.display(ipd.Audio(waveform.numpy(), rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually/randomly split a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "full_dataset = SpeechCommandsDataset(\n",
    "    subset = None,\n",
    "    transform = transform\n",
    ")\n",
    "\n",
    "VAL_SIZE = 0.3\n",
    "\n",
    "# Get train / val split for data points\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "# split the indices\n",
    "first_indices, second_indices = torch.utils.data.random_split(\n",
    "    # provide all indices as a range\n",
    "    range(len(full_dataset)),\n",
    "    # provide percentages (.3, .7)\n",
    "    [1 - VAL_SIZE, VAL_SIZE],\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "# use the indices to select data from the full dataset\n",
    "first_dataset_subset = torch.utils.data.Subset(full_dataset, first_indices)\n",
    "second_dataset_subset = torch.utils.data.Subset(full_dataset, second_indices)\n",
    "\n",
    "# sanity check\n",
    "l_full = len(full_dataset)\n",
    "l_first = len(first_dataset_subset)\n",
    "l_second = len(second_dataset_subset)\n",
    "print(l_full, l_first, l_second, l_full == l_first + l_second)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to change [the transform to be an MFCC](https://pytorch.org/audio/main/generated/torchaudio.transforms.MFCC.html) (Mel-Frequence Cepstrum Coefficient) instead of training the network on a raw waveform. There are a couple of ways of doing this:\n",
    "- [Change the downsampling transform](https://pytorch.org/audio/main/generated/torchaudio.transforms.MFCC.html) to calculate an MFCC instead. Then in the collate function (`audio_folder_collate_fn.py`) you can flatten the 2D MFCC respresentation into a 1D vector before all the vectors then get padded to the same length. This way you can continue to use the 1D CNN in this notebook. \n",
    "- Keep the downsampling transform the same. Instead, calculate the MFCC in the collate function after padding has been applied (`audio_folder_collate_fn.py`), this way all the MFCCs will have the same dimensionality. Instead of flattening the MFCC matricies into vectors, you can then repace the [1D CNN code](#define-the-network) with a 2-D CNN instead and train a classifier on the MFCC matricies (You can borrow coe from Week 3 for this).\n",
    "- Keep the architecture a 1D CNN, and use the MFCC frequencies as channels (see [this ChatGPT thread](https://chatgpt.com/share/6970bc42-69b8-8005-bcf2-80a995e3408d))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
