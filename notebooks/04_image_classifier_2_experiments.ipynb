{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classifier: Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook expects you to have previously trained the MNIST model and saved the resulting file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canvas Installation: Two Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Jupyter (locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommended way is to clone the repo, and install the entire environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Google Colab you will need to use `pip` and install additional libraries (based on [this](https://github.com/pygobject/pycairo/issues/39#issuecomment-391830334)):\n",
    "\n",
    "```bash\n",
    "# WARNING!!!! Do NOT do this if you are running jupyter/python locally!!!\n",
    "!apt-get install libcairo2-dev libjpeg-dev libgif-dev\n",
    "!pip install pycairo py5canvas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Working with the repo in your drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount your drive and change to the correct directory:\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# change directory using the os module\n",
    "import os\n",
    "os.chdir('drive/My Drive/')\n",
    "os.listdir()             # shows the contents of the current dir, you can use chdir again after that\n",
    "# os.mkdir(\"DMLCP-2023\") # creating a directory\n",
    "# os.chdir(\"DMLCP-2023\") # moving to this directory\n",
    "# os.getcwd()            # printing the current directory\n",
    "```\n",
    "\n",
    "See [this notebook](https://colab.research.google.com/notebooks/io.ipynb), and [Working With Files](https://realpython.com/working-with-files-in-python/) on Real Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Working on it as a standalone notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unzip the necessary images with:\n",
    "\n",
    "```python\n",
    "!curl -O https://raw.githubusercontent.com/jchwenger/DMLCP/main/notebooks/images/3.png\n",
    "!curl -O https://raw.githubusercontent.com/jchwenger/DMLCP/main/notebooks/images/4.png\n",
    "!mkdir images\n",
    "!mv 3.png 4.png images\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision as tv\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import py5canvas as cnv\n",
    "\n",
    "# Get cpu, gpu or mps device for training\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "INPUT_SHAPE = [1,28,28]\n",
    "\n",
    "MODELS_DIR = pathlib.Path(\"models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"mnist_dense\"\n",
    "MNIST_DIR = MODELS_DIR / MODEL_NAME\n",
    "\n",
    "GENERATED_DIR = pathlib.Path(\"generated\")\n",
    "GENERATED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MNIST_GEN_DIR = GENERATED_DIR / f\"{MODEL_NAME}_images\"\n",
    "MNIST_GEN_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "model = torch.jit.load(MNIST_DIR / f\"{MODEL_NAME}_scripted.pt\", map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you saved using `torch.save` instead of `torch.jit.save`, you need to redefine your model first, then load the weights into it:\n",
    "\n",
    "```python\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten() # [1, 28, 28] -> [1, 28*28]\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(INPUT_SHAPE[1] * INPUT_SHAPE[2], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(MNIST_DIR / f\"{MODEL_NAME}.pt\", weights_only=True))\n",
    "```\n",
    "\n",
    "The `jit` only method is ideal for using model (inference), **however**, if you want to finetune your model after reloading it, prefer the full method above (class definition + loading weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify an image of a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('images/3.png') # try also images/4.png\n",
    "\n",
    "transforms = v2.Compose([  \n",
    "    tv.transforms.Grayscale(num_output_channels=1),\n",
    "    tv.transforms.Resize(size=(28,28), antialias=True),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "img_transformed = transforms(img)\n",
    "img_transformed = img_transformed.to(device)\n",
    "\n",
    "print(f\"Input shape: {img_transformed.shape}\")\n",
    "\n",
    "def predict(model, img_batch): \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = F.softmax(model(img_batch), dim=-1).cpu().numpy()\n",
    "        return np.argmax(probs[0])\n",
    "        \n",
    "predicted = predict(model, img_transformed)\n",
    "cnv.canvas.show_image(img, title=f'Predicted number: {predicted}', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **disrupt**: try and find cases where the network fails to predict the images properly\n",
    "2. **generate**: come up with your own images and try to classify them! Combining the two, you can try to generate images that the network fails to classify!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Dense vs ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you tried to train a ConvNet, you will notice that it tends to be more stable in its prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Disrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide you with a canvas object that generates images with a number. You can see that a Dense net not always succeeds (and the ConvNet does)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random number between 0 and 9 (the max is excluded)\n",
    "number = np.random.randint(0, 10) \n",
    "c = cnv.Canvas(28, 28)\n",
    "c.background(0)\n",
    "c.fill(255)\n",
    "c.text_size(26)\n",
    "c.text([c.width/2, c.height/2 + 9], str(number), center=True)\n",
    "x = c.get_image_grayscale()\n",
    "# little things:\n",
    "# convert to float32, and convert \n",
    "x = np.array(x)\n",
    "print(x.shape, x.dtype)\n",
    "x = torch.tensor(x, dtype=torch.float32).view(INPUT_SHAPE).to(device)\n",
    "print(x.shape, x.dtype)\n",
    "\n",
    "predicted = predict(model, x)\n",
    "c.show_plt(title=f'Predicted number: {predicted}', size=(512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disruption, first idea: how about we invert the colours? We do that by adding: `1.0 - c.get_image_grayscale()` (our pixel values lie between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = np.random.randint(0, 10)\n",
    "c = cnv.Canvas(28, 28)\n",
    "c.background(255)\n",
    "c.fill(0)\n",
    "c.text_size(26)\n",
    "c.text(str(number), [c.width/2, c.height/2 + 9], center=True)\n",
    "\n",
    "# test: rotation?\n",
    "# c.translate(c.width/2, c.height/2 + 7)\n",
    "# c.rotate(torch.rand(1).item() * 2 * math.pi) # random rotation from 0 to 2 pi\n",
    "# c.text([0, 0], str(number), center=True)\n",
    "img = np.array(c.get_image_grayscale())\n",
    "x = 1.0 - img # Inverted (note: this array has already values in [0,1], no need to divide by 255)\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float32).view(INPUT_SHAPE).to(device)\n",
    "\n",
    "predicted = predict(model, x)\n",
    "c.show_plt(title=f'Predicted number: {predicted}', size=(512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creatively disrupt the image, keeping it recognizable to a human, but causing the model to produce an incorrect prediction. You could add random dots, or patches, for instance. Or simply create an array of random numbers of the same size as the image and add it to the image.\n",
    "- Try to do this in steps, e.g. incrementally adding modifications to the image and observing when and how it stops being recongized by the model.\n",
    "- Briefly discuss the steps you are taking, taking advantage of the hybrid markdown/code format of the notebook.\n",
    "\n",
    "Make sure to display the images you are creaating!\n",
    "\n",
    "You may want to work with the `Canvas` object directly, using some tools demonstrated in the relevant notebook, in which case you should keep in mind that you are only producing grayscale images and that the images have size 28x28.\n",
    "\n",
    "Otherwise you might as well work by preparing images externally (e.g. by hand, or using p5js) and then loading these as we have seen earlier for the image of a four. If you take this approach, make sure you start from an image that is consistently recognizable to a human as a given number and correctly classified by the model as that same number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example that looks like a `0`, and usually gets misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cnv.Canvas(28, 28)\n",
    "c.background(0)\n",
    "\n",
    "c.no_stroke()\n",
    "# try different numbers\n",
    "for t in np.linspace(1, .2, 5):\n",
    "    c.fill(255*t)\n",
    "    # we can also try without a fill, but a stroke instead\n",
    "    # c.no_fill()\n",
    "    # c.stroke(255*t)\n",
    "    c.circle([c.width/2, c.height/2], 10*t)\n",
    "\n",
    "x = np.array(c.get_image_grayscale())\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float32).view(INPUT_SHAPE).to(device)\n",
    "\n",
    "predicted = predict(model, x)\n",
    "c.show_plt(title=f'Predicted number: {predicted}', size=(512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This most interesting when not using the text function any more, but rather using the drawing abilities of canvas.\n",
    "\n",
    "Try different numbers!\n",
    "\n",
    "**Also**, try shapes that *really do not look like numbers* to us, and see what happens.\n",
    "\n",
    "As before, a ConvNet will probably perform better than a plain Dense net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you trained a net on FashionMNIST, you can do the same thing but with pieces of clothing! (The images must always be b&w, 28*28!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optional: fine-tune images!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This requires you to install `imageio`:\n",
    "\n",
    "```python\n",
    "# or pip install imageio\n",
    "conda install -c conda-forge imageio\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import base64\n",
    "import mimetypes\n",
    "import imageio as iio\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Function to save image as a frame\n",
    "def save_image(tensor_img, iteration, run_dir):\n",
    "    img = tensor_img.squeeze().detach().cpu().numpy()\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Save each image to a file\n",
    "    file_path = run_dir / f'frame_{iteration}.png'\n",
    "    plt.savefig(file_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    # Return image path to later convert to a gif\n",
    "    return file_path\n",
    "\n",
    "# annoying business sorting text files numerically (rather than alphabetically)\n",
    "# https://stackoverflow.com/a/4836734\n",
    "def natural_sort(l):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', str(key))]\n",
    "    return sorted(l, key=alphanum_key)\n",
    "\n",
    "# adapted from here: https://github.com/tensorflow/docs/blob/master/tools/tensorflow_docs/vis/embed.py\n",
    "def embed_data(mime, data):\n",
    "    \"\"\"Embeds data as an html tag with a data-url.\"\"\"\n",
    "    b64 = base64.b64encode(data).decode()\n",
    "    if mime.startswith('image'):\n",
    "        tag = f'<img src=\"data:{mime};base64,{b64}\"/>'\n",
    "    elif mime.startswith('video'):\n",
    "        tag = textwrap.dedent(f\"\"\"\n",
    "            <video width=\"640\" height=\"480\" controls>\n",
    "              <source src=\"data:{mime};base64,{b64}\" type=\"video/mp4\">\n",
    "              Your browser does not support the video tag.\n",
    "            </video>\n",
    "            \"\"\")\n",
    "    else:\n",
    "        raise ValueError('Images and Video only.')\n",
    "    return HTML(tag)\n",
    "\n",
    "def embed_file(path):\n",
    "    \"\"\"Embeds a file in the notebook as an html tag with a data-url.\"\"\"\n",
    "    path = pathlib.Path(path)\n",
    "    mime, unused_encoding = mimetypes.guess_type(str(path))\n",
    "    data = path.read_bytes()\n",
    "    return embed_data(mime, data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exploits a key idea in generative deep learning: using the same technique of computing the influence of each parameters on our loss, but this time the **pixels** of the image are the 'parameters' that we modify (whilst the model parameters remain fixed!\n",
    "\n",
    "This *definitely* doesn't work as smoothly as I would want it to (some classes don't produce very recognisable results). Maybe a ConvNet would work better? Or some small detail in there might lead to improvements, experiments required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_image(\n",
    "    chosen_class = 0,        # Chosen class (we'll optimize for '0' which corresponds to class 0)\n",
    "    noise_type = \"uniform\",  # 'uniform' or 'normal'\n",
    "    iters = 100,             # number of iterations (this can be tuned)\n",
    "    learning_rate=0.01,      # learning rate\n",
    "    renorm = True,           # perform (img - img.min())/(img.max - img.min) at each step\n",
    "    clamp = False,           # perform img.clamp_(0, 1) at each step\n",
    "    print_every = 5,         # print loss\n",
    "    show_every = 20,         # show plot/image\n",
    "    save_every = 1,          # save intermediate result\n",
    "    generate_gif = True,     # generate GIF\n",
    "):\n",
    "    \n",
    "    if noise_type == \"uniform\":\n",
    "        # using uniform noise\n",
    "        image = torch.rand(1, 1, 28, 28, device=device, requires_grad=True)\n",
    "    elif noise_type == \"normal\":\n",
    "        # using gaussian noise\n",
    "        image = torch.normal(mean=.5, std=.1, size=(1, 1, 28, 28), device=device, requires_grad=True)\n",
    "        image.data = image.data.clamp(0, 1) # Ensure values are within the [0, 1] \n",
    "\n",
    "    # Define optimizer (we'll use gradient ascent, so we'll update image's pixel values)\n",
    "    optimizer = torch.optim.Adam([image], lr=learning_rate)\n",
    "\n",
    "    # List to store frames for the gif (I save them instead)\n",
    "    # frames = []\n",
    "\n",
    "    # only create a directory if we save\n",
    "    if save_every <= iters:\n",
    "        now = datetime.now().strftime(\"%m-%d-%Y_%Hh%Mm%Ss\")\n",
    "        CURRENT_RUN = MNIST_GEN_DIR / now\n",
    "        CURRENT_RUN.mkdir(exist_ok=True)\n",
    "\n",
    "    # Training loop for gradient ascent\n",
    "    for i in range(iters):\n",
    "\n",
    "        # Normalize the data between 0 and 1 to keep it a valid image\n",
    "        with torch.no_grad():          \n",
    "            if renorm:\n",
    "                image -= image.min()\n",
    "                image /= image.max() - image.min()\n",
    "            if clamp:\n",
    "                image.clamp_(0, 1)\n",
    "        # print(f\"min: {image.data.min():.5f}, max: {image.data.max():.5f}\")\n",
    "    \n",
    "        # 1: prediction\n",
    "        output = model(image)\n",
    "    \n",
    "        # 2: loss (negative on our class, we want to *maximize* the pixels that activate the class)\n",
    "        loss = - output[0, chosen_class]\n",
    "        \n",
    "        # # (positive on all the rest, *exclude* other classes from prediction)\n",
    "        # # (trick: torch.arange(10) != chosen_class is an array of booleans used as indices\n",
    "        # loss = output[0, torch.arange(10) != chosen_class].sum()\n",
    "    \n",
    "        # 3: 'backward' | Backpropagation *on the image*!\n",
    "        loss.backward()\n",
    "    \n",
    "        # 4: 'step'\n",
    "        optimizer.step()\n",
    "    \n",
    "        # 5: 'zero grad' (otherwise the gradients remain there)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Print every `print_every` iterations\n",
    "        if (i+1) % print_every == 0 or i == iters - 1:\n",
    "            print(f\"Iteration {i+1:>{len(str(iters))}}, Loss: {loss.item():.5f}\")\n",
    "            \n",
    "        # Save the intermediate images every `save_every` iterations\n",
    "        if i > 0 and i % save_every == 0:\n",
    "            image_path = save_image(image, i, CURRENT_RUN)\n",
    "            # frames.append(image_path)\n",
    "    \n",
    "        # Plot probs every `show_every` iterations\n",
    "        if i % show_every == 0 or i == iters - 1:\n",
    "            with torch.no_grad():\n",
    "                probs = F.softmax(output, dim = -1).squeeze().detach().cpu()\n",
    "                fig, axs = plt.subplots(1, 2, figsize=(4,2))\n",
    "                axs[0].bar(range(10), probs)\n",
    "                axs[0].set_xticks(range(10))\n",
    "                im = torch.permute(image.detach().cpu().squeeze(dim=0), (1,2,0))\n",
    "                # https://stackoverflow.com/a/10546220\n",
    "                axs[1].imshow(im, cmap=\"gray\", interpolation=\"nearest\", aspect=\"auto\")\n",
    "                axs[1].set_xticks([])\n",
    "                axs[1].set_yticks([])\n",
    "                plt.show()  \n",
    "\n",
    "    # Create a GIF from the frames using imageio\n",
    "    if generate_gif and save_every <= iters:\n",
    "        MNIST_GEN_GIF = MNIST_GEN_DIR / f\"{now}.class_{chosen_class}.gif\"\n",
    "        with iio.get_writer(MNIST_GEN_GIF, mode=\"I\", loop=0, duration=0.1) as writer:\n",
    "            for f in natural_sort(CURRENT_RUN.glob(\"frame_*.png\")):\n",
    "                image = iio.v3.imread(f)\n",
    "                writer.append_data(image)       \n",
    "        print(f\"GIF saved as {MNIST_GEN_GIF}\")\n",
    "        return embed_file(MNIST_GEN_GIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_image(chosen_class=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something that could be nice to do would be to transform this code so that instead of working with only one image, one would use a batch of 10 images, and optimise the loss for each according to its class, and plot a grid of all 9 images in one go!\n",
    "\n",
    "Also, for a savagely awesome example of this process, check out [the end of this notebook](https://github.com/johnowhitaker/aiaiart/blob/master/AIAIART_1.ipynb) ([YT Video](https://youtu.be/p814BapRq2U?si=wD-wtcQqB77EjSVY&t=2821))."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "org": null,
  "vscode": {
   "interpreter": {
    "hash": "1c544d3133b9d8c6f36fca025551af31afa9ef134259e7064ad6be0c15e6401c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
