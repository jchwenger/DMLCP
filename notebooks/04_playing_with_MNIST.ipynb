{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook expects you to have previously trained the MNIST model and saved the resulting file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canvas Installation: Two Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Jupyter (locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommended way is to clone the repo, and install the entire environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Google Colab you will need to use `pip` and install additional libraries (based on [this](https://github.com/pygobject/pycairo/issues/39#issuecomment-391830334)):\n",
    "\n",
    "```bash\n",
    "# WARNING!!!! Do NOT do this if you are running jupyter/python locally!!!\n",
    "!apt-get install libcairo2-dev libjpeg-dev libgif-dev\n",
    "!pip install pycairo py5canvas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Working with the repo in your drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount your drive and change to the correct directory:\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# change directory using the os module\n",
    "import os\n",
    "os.chdir('drive/My Drive/')\n",
    "os.listdir()             # shows the contents of the current dir, you can use chdir again after that\n",
    "# os.mkdir(\"DMLCP-2023\") # creating a directory\n",
    "# os.chdir(\"DMLCP-2023\") # moving to this directory\n",
    "# os.getcwd()            # printing the current directory\n",
    "```\n",
    "\n",
    "See [this notebook](https://colab.research.google.com/notebooks/io.ipynb), and [Working With Files](https://realpython.com/working-with-files-in-python/) on Real Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Working on it as a standalone notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unzip the necessary images with:\n",
    "\n",
    "```python\n",
    "!curl -O https://raw.githubusercontent.com/jchwenger/DMLCP/main/notebooks/images/3.png\n",
    "!curl -O https://raw.githubusercontent.com/jchwenger/DMLCP/main/notebooks/images/4.png\n",
    "!mkdir images\n",
    "!mv 3.png 4.png images\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import canvas\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision as tv\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# Get cpu, gpu or mps device for training\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "INPUT_SHAPE = [1,28,28]\n",
    "\n",
    "MODELS_DIR = pathlib.Path(\"models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"dense_mnist\"\n",
    "MNIST_DIR = MODELS_DIR / MODEL_NAME\n",
    "\n",
    "GENERATED_DIR = pathlib.Path(\"generated\")\n",
    "GENERATED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MNIST_GEN_DIR = GENERATED_DIR / f\"{MODEL_NAME}_images\"\n",
    "MNIST_GEN_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "model = torch.jit.load(MNIST_DIR / f\"{MODEL_NAME}_scripted.pt\", map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you saved using `torch.save` instead of `torch.jit.save`, you need to redefine your model first, then load the weights into it:\n",
    "\n",
    "```python\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten() # [1, 28, 28] -> [1, 28*28]\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(INPUT_SHAPE[1] * INPUT_SHAPE[2], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(MNIST_DIR / f\"{MODEL_NAME}.pt\", weights_only=True))\n",
    "```\n",
    "\n",
    "The `jit` only method is ideal for using model (inference), **however**, if you want to finetune your model after reloading it, prefer the full method above (class definition + loading weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify an image of a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('images/3.png') # try also images/4.png\n",
    "\n",
    "transforms = v2.Compose([  \n",
    "    tv.transforms.Grayscale(num_output_channels=1),\n",
    "    tv.transforms.Resize(size=(28,28), antialias=True),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "input = transforms(img)\n",
    "input = input.to(device)\n",
    "\n",
    "print(f\"Input shape: {input.shape}\")\n",
    "\n",
    "def predict(model, input): \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = nn.Softmax(dim=-1)(model(input)).cpu().numpy()\n",
    "        return np.argmax(probs[0])\n",
    "        \n",
    "predicted = predict(model, input)\n",
    "canvas.show_image(img, title=f'Predicted number: {predicted}', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **disrupt**: try and find cases where the network fails to predict the images properly\n",
    "2. **generate**: come up with your own images and try to classify them! Combining the two, you can try to generate images that the network fails to classify!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Dense vs ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you tried to train a ConvNet, you will notice that it tends to be more stable in its prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Disrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide you with a canvas object that generates images with a number. You can see that a Dense net not always succeeds (and the ConvNet does)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random number between 0 and 9 (the max is excluded)\n",
    "number = np.random.randint(0, 10) \n",
    "c = canvas.Canvas(28, 28)\n",
    "c.background(0)\n",
    "c.fill(255)\n",
    "c.text_size(26)\n",
    "c.text([c.width/2, c.height/2 + 9], str(number), center=True)\n",
    "x = c.get_image_grayscale()\n",
    "\n",
    "# little things:\n",
    "# convert to float32, and convert \n",
    "print(x.shape, x.dtype)\n",
    "x = torch.tensor(x, dtype=torch.float32).view(INPUT_SHAPE).to(device)\n",
    "print(x.shape, x.dtype)\n",
    "\n",
    "predicted = predict(model, x)\n",
    "c.show(title=f'Predicted number: {predicted}', size=(512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disruption, first idea: how about we invert the colours? We do that by adding: `1.0 - c.get_image_grayscale()` (our pixel values lie between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = np.random.randint(0, 10)\n",
    "c = canvas.Canvas(28, 28)\n",
    "c.background(255)\n",
    "c.fill(0)\n",
    "c.text_size(26)\n",
    "c.text([c.width/2, c.height/2 + 9], str(number), center=True)\n",
    "\n",
    "# test: rotation?\n",
    "# c.translate(c.width/2, c.height/2 + 7)\n",
    "# c.rotate(torch.rand(1).item() * 2 * math.pi) # random rotation from 0 to 2 pi\n",
    "# c.text([0, 0], str(number), center=True)\n",
    "\n",
    "x = 1.0 - c.get_image_grayscale() # Inverted (note: this array has already values in [0,1], no need to divide by 255)\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float32).view(INPUT_SHAPE).to(device)\n",
    "\n",
    "predicted = predict(model, x)\n",
    "c.show(title=f'Predicted number: {predicted}', size=(512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creatively disrupt the image, keeping it recognizable to a human, but causing the model to produce an incorrect prediction. You could add random dots, or patches, for instance. Or simply create an array of random numbers of the same size as the image and add it to the image.\n",
    "- Try to do this in steps, e.g. incrementally adding modifications to the image and observing when and how it stops being recongized by the model.\n",
    "- Briefly discuss the steps you are taking, taking advantage of the hybrid markdown/code format of the notebook.\n",
    "\n",
    "Make sure to display the images you are creaating!\n",
    "\n",
    "You may want to work with the `Canvas` object directly, using some tools demonstrated in the relevant notebook, in which case you should keep in mind that you are only producing grayscale images and that the images have size 28x28.\n",
    "\n",
    "Otherwise you might as well work by preparing images externally (e.g. by hand, or using p5js) and then loading these as we have seen earlier for the image of a four. If you take this approach, make sure you start from an image that is consistently recognizable to a human as a given number and correctly classified by the model as that same number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example that looks like a `0`, and usually gets classified as one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = canvas.Canvas(28, 28)\n",
    "c.background(0)\n",
    "\n",
    "c.no_stroke()\n",
    "for t in np.linspace(1, 0.2, 5):\n",
    "    c.fill(255*t)\n",
    "    c.circle([c.width/2, c.height/2], 10*t)\n",
    "\n",
    "x = c.get_image_grayscale()\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float32).view(INPUT_SHAPE).to(device)\n",
    "\n",
    "predicted = predict(model, x)\n",
    "c.show(title=f'Predicted number: {predicted}', size=(512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This most interesting when not using the text function any more, but rather using the drawing abilities of canvas.\n",
    "\n",
    "Try different numbers!\n",
    "\n",
    "**Also**, try shapes that *really do not look like numbers* to us, and see what happens.\n",
    "\n",
    "As before, a ConvNet will probably perform better than a plain Dense net.\n",
    "\n",
    "### Note\n",
    "\n",
    "If you trained a net on FashionMNIST, you can do the same thing but with pieces of clothing! (The images must always be b&w, 28*28!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optional: fine-tune images!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This requires you to install `imageio`:\n",
    "\n",
    "```python\n",
    "# or pip install imageio\n",
    "conda install -c conda-forge imageio\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import mimetypes\n",
    "import imageio as iio\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Function to save image as a frame\n",
    "def save_image(tensor_img, iteration):\n",
    "    img = tensor_img.squeeze().detach().cpu().numpy()\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Save each image to a file\n",
    "    file_path = CURRENT_RUN / f'frame_{iteration}.png'\n",
    "    plt.savefig(file_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    # Return image path to later convert to a gif\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exploits a key idea in generative deep learning: using the same technique of computing the influence of each parameters on our loss, but this time the **pixels** of the image are the 'parameters' that we modify (whilst the model parameters remain fixed!\n",
    "\n",
    "This *definitely* doesn't work as smoothly as I would want it to (some classes don't produce very recognisable results). Maybe a ConvNet would work better? Or some small detail in there might lead to improvements, experiments required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Chosen class (we'll optimize for '0' which corresponds to class 0)\n",
    "CHOSEN_CLASS = 0\n",
    "\n",
    "# Initialize a random image of size (28, 28) with values between 0 and 1\n",
    "noise = True\n",
    "if noise == True:\n",
    "    # using uniform noise\n",
    "    image = torch.rand(1, 1, 28, 28, device=device, requires_grad=True)\n",
    "else:\n",
    "    # using gaussian noise\n",
    "    image = torch.normal(mean=.5, std=.1, size=(1, 1, 28, 28), device=device, requires_grad=True)\n",
    "    image.data = image.data.clamp(0, 1) # Ensure values are within the [0, 1] \n",
    "\n",
    "# Define optimizer (we'll use gradient ascent, so we'll update image's pixel values)\n",
    "optimizer = torch.optim.Adam([image], lr=0.001)\n",
    "\n",
    "# Number of iterations (this can be tuned)\n",
    "iters = 5000\n",
    "\n",
    "# print loss & show plot/image\n",
    "PRINT_EVERY = 200\n",
    "SHOW_EVERY = 1000\n",
    "\n",
    "# List to store frames for the gif (I save them instead)\n",
    "# frames = []\n",
    "\n",
    "# When do we save the intermediate result\n",
    "SAVE_EVERY = 10\n",
    "\n",
    "# only create a directory if we save\n",
    "if SAVE_EVERY <= iters:\n",
    "    now = datetime.now().strftime(\"%m-%d-%Y_%Hh%Mm%Ss\")\n",
    "    CURRENT_RUN = MNIST_GEN_DIR / now\n",
    "    CURRENT_RUN.mkdir(exist_ok=True)\n",
    "\n",
    "# Training loop for gradient ascent\n",
    "for i in range(iters):\n",
    "\n",
    "    # # Rescale & inject some noise into our data (both optional)\n",
    "    # image.data = image.data * .6 + torch.rand(1, 1, 28, 28, device=device) * .02\n",
    "\n",
    "    # Normalize the data between 0 and 1\n",
    "    image.data = (image.data - image.data.min()) / (image.data.max() - image.data.min())\n",
    "\n",
    "    # # Other option: standardize the data (mean 0, std 1)\n",
    "    # image.data = (image.data - torch.mean(image)) / torch.std(image) * 0.15 + .5\n",
    "    \n",
    "    # Clamp the pixel values between 0 and 1 to keep it a valid image\n",
    "    image.data.clamp_(0, 1)  \n",
    "\n",
    "    # 1: prediction\n",
    "    output = model(image)\n",
    "\n",
    "    # 2: loss\n",
    "    # (negative on our class, we want to *maximize* the pixels that activate the class)\n",
    "    loss = - output[0, CHOSEN_CLASS]\n",
    "    \n",
    "    # # (positive on all the rest, *exclude* other classes from prediction)\n",
    "    # # (trick: torch.arange(10) != CHOSEN_CLASS is an array of booleans used as indices\n",
    "    # loss = output[0, torch.arange(10) != CHOSEN_CLASS].sum()\n",
    "\n",
    "    # 3: 'backward' | Backpropagation *on the image*!\n",
    "    loss.backward()\n",
    "\n",
    "    # 4: 'step'\n",
    "    optimizer.step()\n",
    "\n",
    "    # 5: 'zero grad' (otherwise the gradients remain there)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Print every `PRINT_EVERY` iterations\n",
    "    if (i+1) % PRINT_EVERY == 0 or i == iters - 1:\n",
    "        print(f\"Iteration {i+1:>{len(str(iters))}}, Loss: {loss.item():.5f}\")\n",
    "        \n",
    "    # Save the intermediate images every `SAVE_EVERY` iterations\n",
    "    if i > 0 and i % SAVE_EVERY == 0:\n",
    "        image_path = save_image(image, i)\n",
    "        # frames.append(image_path)\n",
    "\n",
    "    # Plot probs every `SHOW_EVERY` iterations\n",
    "    if i % SHOW_EVERY == 0 or i == iters - 1:\n",
    "        with torch.no_grad():\n",
    "            probs = F.softmax(output, dim = -1).squeeze().detach().cpu()\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(4,2))\n",
    "            axs[0].bar(range(10), probs)\n",
    "            axs[0].set_xticks(range(10))\n",
    "            im = torch.permute(image.detach().cpu().squeeze(dim=0), (1,2,0))\n",
    "            # https://stackoverflow.com/a/10546220\n",
    "            axs[1].imshow(im, cmap=\"gray\", interpolation=\"nearest\", aspect=\"auto\")\n",
    "            axs[1].set_xticks([])\n",
    "            axs[1].set_yticks([])\n",
    "            plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annoying business sorting text files numerically (rather than alphabetically)\n",
    "# https://stackoverflow.com/a/4836734\n",
    "import re\n",
    "def natural_sort(l):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', str(key))]\n",
    "    return sorted(l, key=alphanum_key)\n",
    "\n",
    "# Create a GIF from the frames using imageio\n",
    "MNIST_GEN_GIF = MNIST_GEN_DIR / f\"{now}.class_{CHOSEN_CLASS}.gif\"\n",
    "with iio.get_writer(MNIST_GEN_GIF, mode=\"I\", loop=0, duration=0.1) as writer:\n",
    "    for f in natural_sort(CURRENT_RUN.glob(\"frame_*.png\")):\n",
    "        image = iio.v3.imread(f)\n",
    "        writer.append_data(image)\n",
    "        \n",
    "print(f\"GIF saved as {MNIST_GEN_GIF}\")\n",
    "\n",
    "# adapted from here: https://github.com/tensorflow/docs/blob/master/tools/tensorflow_docs/vis/embed.py\n",
    "\n",
    "def embed_data(mime, data):\n",
    "    \"\"\"Embeds data as an html tag with a data-url.\"\"\"\n",
    "    b64 = base64.b64encode(data).decode()\n",
    "    if mime.startswith('image'):\n",
    "        tag = f'<img src=\"data:{mime};base64,{b64}\"/>'\n",
    "    elif mime.startswith('video'):\n",
    "        tag = textwrap.dedent(f\"\"\"\n",
    "            <video width=\"640\" height=\"480\" controls>\n",
    "              <source src=\"data:{mime};base64,{b64}\" type=\"video/mp4\">\n",
    "              Your browser does not support the video tag.\n",
    "            </video>\n",
    "            \"\"\")\n",
    "    else:\n",
    "        raise ValueError('Images and Video only.')\n",
    "    return HTML(tag)\n",
    "\n",
    "def embed_file(path):\n",
    "    \"\"\"Embeds a file in the notebook as an html tag with a data-url.\"\"\"\n",
    "    path = pathlib.Path(path)\n",
    "    mime, unused_encoding = mimetypes.guess_type(str(path))\n",
    "    data = path.read_bytes()\n",
    "    return embed_data(mime, data)\n",
    "\n",
    "embed_file(MNIST_GEN_GIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something that could be nice to do would be to transform this code so that instead of working with only one image, one would use a batch of 10 images, and optimise the loss for each according to its class, and plot a grid of all 9 images in one go!\n",
    "\n",
    "Also, for a savagely awesome example of this process, check out [the end of this notebook](https://github.com/johnowhitaker/aiaiart/blob/master/AIAIART_1.ipynb) ([YT Video](https://youtu.be/p814BapRq2U?si=wD-wtcQqB77EjSVY&t=2821))."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "org": null,
  "vscode": {
   "interpreter": {
    "hash": "1c544d3133b9d8c6f36fca025551af31afa9ef134259e7064ad6be0c15e6401c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
