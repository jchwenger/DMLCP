{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06LmUlDIrKz",
    "outputId": "557bdeef-ae08-4cee-92ce-9afc46bbb34a"
   },
   "source": [
    "# Language Models 1 | Inference (Huggingface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06LmUlDIrKz",
    "outputId": "557bdeef-ae08-4cee-92ce-9afc46bbb34a"
   },
   "source": [
    "For more, see [here](https://huggingface.co/tasks/text-generation) and [here](https://huggingface.co/docs/transformers/generation_strategies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06LmUlDIrKz",
    "outputId": "557bdeef-ae08-4cee-92ce-9afc46bbb34a"
   },
   "source": [
    "## Install & Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06LmUlDIrKz",
    "jp-MarkdownHeadingCollapsed": true,
    "outputId": "557bdeef-ae08-4cee-92ce-9afc46bbb34a"
   },
   "source": [
    "#### Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06LmUlDIrKz",
    "outputId": "557bdeef-ae08-4cee-92ce-9afc46bbb34a"
   },
   "source": [
    "If you need to load/save to your drive:\n",
    "\n",
    "```python\n",
    "import sys\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive/\")\n",
    "\n",
    "import os\n",
    "os.chdir(\"drive/My Drive/gold/IS53055B-DMLCP/DMLCP\") # to change to another directory\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06LmUlDIrKz",
    "jp-MarkdownHeadingCollapsed": true,
    "outputId": "557bdeef-ae08-4cee-92ce-9afc46bbb34a"
   },
   "source": [
    "#### Huggingface login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06LmUlDIrKz",
    "outputId": "557bdeef-ae08-4cee-92ce-9afc46bbb34a"
   },
   "source": [
    "For some models and datasets, and if you want to push your model to HF (same as GitHub, but for models) you need to be logged into your HF account.\n",
    "\n",
    "For that, you need to create an account [here](https://huggingface.co/) and then to ['/settings/tokens'](https://huggingface.co/settings/tokens) to create an access token.\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "from huggingface_hub import notebook_login\n",
    "if not (Path.home()/\".huggingface\"/\"token\").exists():\n",
    "    notebook_login()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URjvsuUyIthY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "# See: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html#creating-models\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    # note: models using bfloat16 aren't compatible with MPS\n",
    "    # else \"mps\"\n",
    "    # if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The textwrap module automatically formats text for you\n",
    "import textwrap\n",
    "\n",
    "# many more options, see them with textwrap.TextWrapper?\n",
    "tw = textwrap.TextWrapper(\n",
    "    # the formatted width we want\n",
    "    width=79,\n",
    "    # this will keep whitespace & line breaks in the original text\n",
    "    replace_whitespace=False\n",
    ")\n",
    "\n",
    "def wrap_print(s):\n",
    "    \"\"\"Format text into Textwrapped lines and print it\"\"\"\n",
    "    print(\"\\n\".join(tw.wrap(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-the-box Generation: the `pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline` works for literally everything, so we need to specify the task (`text-generation`), and the model ([so many to choose from...](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads), here's [the current model page](https://huggingface.co/Qwen/Qwen3-0.6B?library=transformers)). We also select the [device](https://huggingface.co/docs/transformers/pipeline_tutorial#device)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"Qwen/Qwen3-0.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9Tf3fn2Z_DV"
   },
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=MODEL_ID,\n",
    "    device=device \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.from_pretrained.example) for an example using `GenerationConfig` and [here](https://github.com/huggingface/transformers/issues/19853#issuecomment-1290759818) for the `pad_token_id` fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sueT3i6MfM8d"
   },
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(MODEL_ID)\n",
    "print(generation_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVIs-VhOQN6O"
   },
   "source": [
    "The Huggingface is transitioning towards the use of generation config files (good for automation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fZaJdUnPg7E"
   },
   "outputs": [],
   "source": [
    "generation_config.max_length = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW2UOpO2QAFD"
   },
   "source": [
    "### Quick vocab note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW2UOpO2QAFD"
   },
   "source": [
    "`bos`: beginning of sentence  \n",
    "`eos`: end of sentence  \n",
    "`pad`: padding\n",
    "\n",
    "These are special tokens that have been inserted into the text at training time.\n",
    "\n",
    "For instance, in our case the 'beginning' of the text is 'endoftext', as during training the texts are fed to the network one after the other, with this special token between them:\n",
    "```python\n",
    "# the number (token id) representing the beginning of sentence special token\n",
    "bos_token_id = generation_config.bos_token_id\n",
    "print(bos_token_id)\n",
    "# decode that number back into a string\n",
    "print(generator.tokenizer.decode([bos_token_id]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61kgReONYBT5",
    "outputId": "cd2bae9a-2feb-44d7-c977-e371fea14c38"
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(1)\n",
    "generator(\n",
    "    \"Once upon a time,\",\n",
    "    generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McOEE98ITaoz"
   },
   "source": [
    "Parallel generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40-Zwu0ETWnW",
    "outputId": "34296884-51a8-46ff-baba-9ac0e1a8ff97"
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(1)\n",
    "generator(\n",
    "    [\"Once upon a time,\"] * 2,\n",
    "    generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3s8ntFfeB30"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3s8ntFfeB30"
   },
   "source": [
    "## Deeper:`Tokenizer` and `Model` classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3s8ntFfeB30"
   },
   "source": [
    "What does the pipeline do under the hood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EzS8SV5ccuu"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# to GPU/MPS/CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXJjd_kEREkS"
   },
   "source": [
    "### The tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXJjd_kEREkS"
   },
   "source": [
    "See [the Preprocess](https://huggingface.co/docs/transformers/preprocessing) tutorial on Huggingface for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddqPQPE_SEOA",
    "outputId": "d0414e3d-8713-4948-b39a-2a5a6ba48419"
   },
   "outputs": [],
   "source": [
    "toks = tokenizer.encode(\"Oh sweet midnight\")\n",
    "print(toks)\n",
    "print(tokenizer.decode(toks))\n",
    "print()\n",
    "\n",
    "toks = tokenizer([\"Oh sweet midnight\", \"harbinger of doom\"])\n",
    "print(toks)\n",
    "print(tokenizer.batch_decode(toks[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gju_L3gKVN4j",
    "outputId": "3914d075-337b-4850-a223-6fb6ac0cf7ad"
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\"Once upon a time\", return_tensors=\"pt\") # pytorch tensors\n",
    "print(input_ids)\n",
    "\n",
    "batched_input_ids = torch.tile(input_ids, (4,1)).to(device) # just copying the tensor 4 times\n",
    "print(batched_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVKQUtN9eEpW"
   },
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on, return pytorch tensors\n",
    "input_ids = tokenizer.encode(\"Once upon a time\", return_tensors=\"pt\")\n",
    "\n",
    "# copy and place on GPU/MPS/CPU\n",
    "batched_input_ids = torch.tile(input_ids, (4,1)).to(device)\n",
    "\n",
    "# same logic as before\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_ID)\n",
    "generation_config.max_length = 100\n",
    "# suppressing a pesky warning (https://stackoverflow.com/a/71397707)\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "output = model.generate(\n",
    "    # try input_ids as well for a single strand\n",
    "    batched_input_ids,\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ibq2IrdhhSAH",
    "outputId": "4f4d119b-a32b-440a-d3ec-144a83d48e1f"
   },
   "outputs": [],
   "source": [
    "texts = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "for t in texts:\n",
    "    wrap_print(t)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat models & templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highly recommended: an [interactive tokenizer web app](https://tiktokenizer.vercel.app/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. With the `pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [Chat basics page](https://huggingface.co/docs/transformers/en/conversations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful science assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hey, can you define gravity in one sentence?\"}\n",
    "]\n",
    "\n",
    "generator = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=MODEL_ID,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "response = generator(chat, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0]: only one batch\n",
    "# inside that, [\"generated_text\"] contains the whole chat\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the whole chat\n",
    "chat = response[0][\"generated_text\"]\n",
    "# add a new user response\n",
    "chat.append(\n",
    "    {\"role\": \"user\", \"content\": \"Woah! But can it be reconciled with quantum mechanics?\"}\n",
    ")\n",
    "\n",
    "# generate again\n",
    "response = generator(chat, max_new_tokens=512)\n",
    "\n",
    "# print the response\n",
    "print(response[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Manual tokenization & formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [Chat template page](https://huggingface.co/docs/transformers/en/chat_templating).\n",
    "\n",
    "In order to make LLMs behave like chatbots is by fine-tuning them on text that follows a specific format, with markers (called \"special tokens\") that won't necessarily be seen by the user, but can be detected internally, and signify \"this is the bot speaking\", \"the reply ends here\", etc. By detecting these particular tokens (remember that they are just numbers under the hood), it is possible e.g. to stop generation whenever the special token for the end-of-reply is detected. \n",
    "\n",
    "Formatting text in this way is called \"templating\", and the scheme for such a thing is a \"chat template\". Each model has its own, even if they are mostly similar. The template for our model can be found [here](https://ollama.com/library/qwen3:0.6b/blobs/ae370d884f10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Applying the full template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to give the model the right format (models behave best when the input is formatted in the same way as the data they have been trained on), we use `tokenizer.apply_chat_template`. Notice how having `add_generation_prompt=True` adds this at the end of our chat:\n",
    "```\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```\n",
    "\n",
    "This changes the text the model reads, and since the only thing it does is continue the text to the best of its abilities, now it will continue in it by generating what the 'assistant' would say.\n",
    "\n",
    "Note also the 'thinking' bit: 'disable thinking' actually just means adding:\n",
    "```\n",
    "<think>\n",
    "\n",
    "</think>\n",
    "```\n",
    "to the text, which means the model will just continue generating what should come after an empty thinking block..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "    tokenize=True,\n",
    "\tadd_generation_prompt=True,\n",
    "\treturn_tensors=\"pt\",\n",
    "    # if True, the model will write preliminary steps in a <think> block before answering\n",
    "    enable_thinking=False\n",
    ").to(model.device)\n",
    "\n",
    "print(tokenizer.decode(tokenized_chat[\"input_ids\"][0]))\n",
    "# the text is surrounded by: '<|im_start|>system' or '<|im_start|>user' and '<|im_end|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**tokenized_chat, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve only the response, you would have to slice manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the answer starts after the length of the input\n",
    "answer = outputs[0, tokenized_chat[\"input_ids\"].shape[1] :]\n",
    "print(tokenizer.decode(answer, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Leaving the template incomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we dont have `add_generation_prompt`, the tokenizer does not add the directing bit at the end:\n",
    "```\n",
    "<|im_start|>assistant\n",
    "<think>\n",
    "\n",
    "</think>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_chat_no_prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "print(tokenized_chat_no_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your use case, you might want that. The model could now continue in a different way than adding an assistant response..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Unclosed message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even go further, and not even have the special 'end' token, meaning that in all likelihood the model will continue the last message. This can be useful if you wanted to put words in the model's mouth (prefilling its response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclosed_chat = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    "    continue_final_message=True\n",
    ").to(device)\n",
    "\n",
    "print(tokenizer.decode(unclosed_chat[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**unclosed_chat, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U18eRLBbXAot"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U18eRLBbXAot"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U18eRLBbXAot"
   },
   "source": [
    "1. Test everything! Make sure you understand and develop an intuition of:\n",
    " - The various parameters: `temperature`, `top_k`, `top_p`;\n",
    " - The `tokenizer` object to convert text into tokens and back;\n",
    " - How to handle the whole pipeline;\n",
    "   Also, you can search for different [models](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads)! (Some of them may exceed your GPU capacity, beware). People have finetuned language models on many types of texts.\n",
    "2. Can you think of a way to introduce computational thinking into this? Ideas:\n",
    "  - First, you could explore ways of making things look nicer? Instead of just having a list of objects? You could write a nice print function that knows exactly how to take the model output and print it in a nice way. The specialised Python package with many text functionalities is [textwrap](https://docs.python.org/3/library/textwrap.html) (see also [here](https://www.geeksforgeeks.org/textwrap-text-wrapping-filling-python/));\n",
    "  - Can you think of ways to construct a writing **loop**? By that, I mean:  \n",
    "    a. Prepare prompt  \n",
    "    b. Generate one or more strands of text  \n",
    "    c. Select text from strands, go back to a.  \n",
    "    This could simply mean writing a system of helper functions and classes to assist you in the writing...\n",
    "  - One could imagine all sorts of strange ways to work with text, from programmatically chunking the generated text and scrambling it before using it again as a prompt, to explore what the model does if you use unreasonable parameters (e.g. a very high or low `temperature`).\n",
    "  - Also, can you think of ways to work with various strands of text (Taking advantage of the fact that a model can generate in parallel)?\n",
    "\n",
    "3. Something that has already been the subject of a lot of debate and controversy, is the exploration of the *biases* of the models (and there are tons!). LLMs are trained mostly on Internet text, top-ranked reddit posts, etc. (see for instance [this open-source replication](https://github.com/jcpeterson/openwebtext)). Unsurprisingly, the topics and points of view reflect that corner of human activities... Differences between American and Chinese models are also interesting to explore."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
