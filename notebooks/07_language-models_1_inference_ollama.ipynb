{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b146c0ef-83b3-4b16-998e-2fd737869315",
   "metadata": {},
   "source": [
    "# Language Models 1 | Inference (Ollama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d5f7f-29fa-4602-8381-8dedcf77d7ed",
   "metadata": {},
   "source": [
    "**For this notebook to run, you need an ollama server running (`ollama serve` in a terminal)!**\n",
    "\n",
    "[ollama.com](https://ollama.com)  \n",
    "[ollama Github doc](https://github.com/ollama/ollama)  \n",
    "[ollama Python doc](https://github.com/ollama/ollama-python)  \n",
    "[markdown doc](https://python-markdown.github.io/reference/)\n",
    "\n",
    "Here we will be using the library [Ollama](https://ollama.com) ([github](https://github.com/ollama/ollama) – itself based on a lower-level project called [llama.cpp](https://github.com/ggml-org/llama.cpp)), to run LLMs. locally. The other place I recommend looking up if you want to know more is [Huggingface](https://huggingface.co/), which is a set of libraries, a hub to share models and datasets, and a provider of many tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12567bf4-2968-40d3-9dcc-b24b879808a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import IPython\n",
    "import numpy as np\n",
    "\n",
    "import markdown\n",
    "import strip_markdown\n",
    "\n",
    "import base64\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6074bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to check if a model is available: if not: download it\n",
    "def check_model_and_pull(m_name):\n",
    "    # test if the model is downloaded, if not pull (download) from the server\n",
    "    if m_name not in [m.model for m in ollama.list().models]:\n",
    "        print(f\"model '{m_name}' not found, downloading...\")\n",
    "        # pull/download model\n",
    "        ollama.pull(m_name)\n",
    "    else:\n",
    "        print(f\"model: `{m_name}` found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb34d93",
   "metadata": {},
   "source": [
    "[Gemma 3 family](https://ollama.com/library/gemma3)  \n",
    "[Gemma 3 270m](https://ollama.com/library/gemma3:270m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bb1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemma3:270m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22dd7ef-2c79-44a9-ab23-3cb0f50e9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if the model is downloaded, if not pull from the server\n",
    "check_model_and_pull(model_name)\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model_name, \n",
    "    messages=[\n",
    "      { \"role\": \"user\", \"content\": \"Why is the sky blue?\" },\n",
    "    ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358924dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2bd1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"message\"][\"content\"])\n",
    "# you can also access fields directly from the response object\n",
    "# print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14359474",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df83415",
   "metadata": {},
   "source": [
    "Full list [here](https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ef014",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{ \"role\": \"user\", \"content\": \"Why is the sky blue?\" },],\n",
    "        options={\n",
    "            \"temperature\": 0.,\n",
    "        })[\"message\"][\"content\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d44bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{ \"role\": \"user\", \"content\": \"Why is the sky blue?\" },],\n",
    "        options={\n",
    "            \"temperature\": 10.,\n",
    "        })[\"message\"][\"content\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984fcecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{ \"role\": \"user\", \"content\": \"Why is the sky blue?\" },],\n",
    "        options={\n",
    "            # stop after a specific number of tokens\n",
    "            \"num_predict\": 5\n",
    "        })[\"message\"][\"content\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86721c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{ \"role\": \"user\", \"content\": \"Why is the sky blue?\" },],\n",
    "        options={\n",
    "            # use a special stopping sequence\n",
    "            \"stop\": [\"a\"],\n",
    "        })[\"message\"][\"content\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e9c20-783e-4e75-9056-fc1a2702d02c",
   "metadata": {},
   "source": [
    "## Note: handle markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0961d9",
   "metadata": {},
   "source": [
    "[Markdown](https://pypi.org/project/Markdown/)  \n",
    "[strip-markdown](https://pypi.org/project/strip-markdown/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73216dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322cbb6-7e7e-422b-973c-404c262c33bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md2html(text):\n",
    "    return markdown.markdown(text)\n",
    "\n",
    "def print_html(raw_html):\n",
    "    IPython.display.display_html(raw_html, raw=True)\n",
    "\n",
    "print_html(md2html(response.message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeda090e-79f7-472e-be09-a6eab2a52923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_md(text):\n",
    "    return strip_markdown.strip_markdown(text)\n",
    "\n",
    "print(strip_md(response.message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5096635-de00-4e34-922f-d3f51e0335c6",
   "metadata": {},
   "source": [
    "## Gradual printing / streaming responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a32f49-5e20-4aa3-8474-b9bae9e5e040",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = ollama.chat(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb04775a-5f39-4664-b239-b37ff24ecfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = ollama.chat(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "text = \"\"\n",
    "for chunk in stream:\n",
    "    text += chunk[\"message\"][\"content\"]\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    print_html(md2html(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0aa30",
   "metadata": {},
   "source": [
    "### Extra: Generate experiment: removing the template!\n",
    "\n",
    "[doc](https://github.com/ollama/ollama-python?tab=readme-ov-file#generate), [REST API](https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c50727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hi, my name is Jeremie.\"\n",
    "\n",
    "print(\n",
    "    ollama.generate(\n",
    "        model=model_name,\n",
    "        prompt=prompt,\n",
    "        # if `True`` no formatting will be applied to the prompt!\n",
    "        raw=True,\n",
    "        options= {\n",
    "            # here we limit the output to 50 tokens only\n",
    "            \"num_predict\": 50,\n",
    "            # you can play with the temperature if you want\n",
    "            # \"temperature\": .9,\n",
    "            }\n",
    "        )[\"response\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e9bafe",
   "metadata": {},
   "source": [
    "Looking at the [template](https://ollama.com/library/gemma3:270m/blobs/4b19ac7dd2fb) (also [test it live in Tiktokenizer!](https://tiktokenizer.vercel.app/?model=google%2Fgemma-7b), an earlier model in this series but the template looks the same), I can manually recreate the text that the model actually reads, so that it behaves again like a chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the structure has these special markers + new lines\n",
    "prompt = \"\"\"<start_of_turn>user\n",
    "Hi, my name is Jeremie.<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "# # if you use this, the model will actually start answering as the *user*!\n",
    "# prompt = \"\"\"<start_of_turn>user\n",
    "# Hi, my name is Jeremie.<end_of_turn>\n",
    "# <start_of_turn>model\n",
    "# Hi, I'm a weird machine, how can I be of help?<end_of_turn>\n",
    "# <start_of_turn>user\n",
    "# \"\"\"\n",
    "\n",
    "print(\n",
    "    ollama.generate(\n",
    "        model=model_name,\n",
    "        prompt= prompt,\n",
    "        # if `True`` no formatting will be applied to the prompt!\n",
    "        raw=True,\n",
    "        options= {\n",
    "            # here we limit the output to 50 tokens only\n",
    "            \"num_predict\": 50,\n",
    "            # \"temperature\": .9,\n",
    "            \n",
    "            }\n",
    "        )[\"response\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4a5df",
   "metadata": {},
   "source": [
    "## Extra: multimodality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a1e8b",
   "metadata": {},
   "source": [
    "[Gemma 3, 4b doc](https://ollama.com/library/gemma3:4b)\n",
    "\n",
    "**Multimodality** refers to the fact that models are trained, and therefore can understand/interact with, multiple types of data: in the most common case, it's *text* and *images* (could also be sound, video, sensory data from a robot, etc.). Here you can see an example of a model that is able to read/understand images.\n",
    "\n",
    "Adapted from the [gemma3 example](https://github.com/ollama/ollama-python/blob/main/examples/multimodal-chat.py) – a bigger model, taking around 2-3GB in RAM. (See also the [llava example](https://github.com/ollama/ollama-python/blob/main/examples/multimodal-generate.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our new model name \n",
    "model_name = \"gemma3:4b\"\n",
    "\n",
    "# download it if not present\n",
    "check_model_and_pull(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38acf331",
   "metadata": {},
   "source": [
    "[base64 doc](https://docs.python.org/3/library/base64.html)  \n",
    "[w<sup>3</sup> tutorial](https://www.w3schools.com/Python/ref_module_base64.asp)  \n",
    "[base64 RealPython tutorial](https://realpython.com/python-serialize-data/)\n",
    "\n",
    "Ollama supports feeding the image as a path, a `base64` string (a kind of encoding, using only ASCII characters), or raw bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_option = 0\n",
    "\n",
    "# pass in the path to the image\n",
    "path_or_img = \"images/spock.jpg\"\n",
    "\n",
    "if file_option == 1:\n",
    "    # you can also pass in base64 encoded image data\n",
    "    path_or_img = base64.b64encode(Path(path_or_img).read_bytes()).decode()\n",
    "elif file_option == 2:\n",
    "    # or the raw bytes\n",
    "    path_or_img = Path(path_or_img).read_bytes()\n",
    "\n",
    "response = ollama.chat(\n",
    "    # note that \n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'What is in this image? Be detailed.',\n",
    "            # we just add an 'image' key:value pair to our message object\n",
    "            # the value is a list, containing one or more images (as paths/base64 str/bytes)\n",
    "            'images': [path_or_img],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print_html(md2html(response.message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5744b62f",
   "metadata": {},
   "source": [
    "### Extra: wanna see the `base64` string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79865fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read raw bytes, then encode as b64 (still bytes), then decode as string\n",
    "image_as_str = base64.b64encode(Path(path_or_img).read_bytes()).decode()\n",
    "# only the first 100 characters\n",
    "image_as_str[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a9d712",
   "metadata": {},
   "source": [
    "## Extra: tools, web browsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f89e04",
   "metadata": {},
   "source": [
    "Recent models have even more functionalities, such as:\n",
    "- using tools/calling functions ([tools example](https://github.com/ollama/ollama-python/blob/main/examples/tools.py), [multi-tool example](https://github.com/ollama/ollama-python/blob/main/examples/multi-tool.py), [async tools](https://github.com/ollama/ollama-python/blob/main/examples/async-tools.py))\n",
    "- 'thinking' (namely generate more tokens to arrive at an answer) ([thinking chat example](https://github.com/ollama/ollama-python/blob/main/examples/thinking.py), [thinking generate example](https://github.com/ollama/ollama-python/blob/main/examples/thinking-generate.py), [thinking levels examples](https://github.com/ollama/ollama-python/blob/main/examples/thinking-levels.py))\n",
    "- web-browsing ([qwen example](https://github.com/ollama/ollama-python/blob/main/examples/web-search.py), [gpt-oss example](https://github.com/ollama/ollama-python/blob/main/examples/web-search-gpt-oss.py))\n",
    "\n",
    "\n",
    "Beware, many recent models require a lot of memory. For example **gpt-oss-20b** requires just under 12GB of RAM..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
