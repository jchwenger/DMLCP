{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSiVr3fyIlGQ"
   },
   "source": [
    "# Language Models 2 | Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSiVr3fyIlGQ"
   },
   "source": [
    "Modified from [Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training).\n",
    "\n",
    "Note, this is *meant to be used on Colab*, simply because you will need a fair amount of GPU memory to get these models running! But if your machine has a GPU or enough RAM, then the course environment works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSiVr3fyIlGQ"
   },
   "source": [
    "## Install & Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSiVr3fyIlGQ"
   },
   "source": [
    "#### Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSiVr3fyIlGQ"
   },
   "source": [
    "If you need to load/save to your drive:\n",
    "\n",
    "```python\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "import os\n",
    "os.chdir('drive/My Drive/gold/IS53055B-DMLCP/DMLCP') # to change to another directory\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSiVr3fyIlGQ"
   },
   "source": [
    "#### Huggingface login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSiVr3fyIlGQ"
   },
   "source": [
    "For some models and datasets, and if you want to push your model to HF (same as GitHub, but for models) you need to be logged into your HF account.\n",
    "\n",
    "For that, you need to create an account [here](https://huggingface.co/) and then to ['/settings/tokens'](https://huggingface.co/settings/tokens) to create an access token.\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "from huggingface_hub import notebook_login\n",
    "if not (Path.home()/'.huggingface'/'token').exists():\n",
    "    notebook_login()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "vcMun0XDn3xU",
    "outputId": "cb84f444-4845-478c-e506-618febe53b6e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "# See: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html#creating-models\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDzalkV7T0x0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textwrap # The textwrap module automatically formats text for you\n",
    "\n",
    "tw = textwrap.TextWrapper(   # many more options, see them with textwrap.TextWrapper?\n",
    "    width=79,                # the formatted width we want\n",
    "    replace_whitespace=False # this will keep whitespace & line breaks in the original text\n",
    ")\n",
    "\n",
    "def wrap_print(s):\n",
    "    \"\"\"Format text into Textwrapped lines and print it\"\"\"\n",
    "    print(\"\\n\".join(tw.wrap(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMcUveahB0pk"
   },
   "source": [
    "## Test a raw generation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMcUveahB0pk"
   },
   "source": [
    "You can choose from a [large amount of tasks](https://huggingface.co/tasks), and, in each task, from a [huge amount of models](https://huggingface.co/models?sort=trending)! Not only that, but you can also find loads and loads of [datasets](https://huggingface.co/datasets?task_categories=task_categories:text-generation&sort=trending) readily available, and already classified by tasks. In our case, we will be using the [`tiny_shakespeare`](https://huggingface.co/datasets/tiny_shakespeare) dataset, originally created by [Andrej Karpathy](https://karpathy.ai/) to test language models.\n",
    "\n",
    "In this case, you can see [here](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending) the list of models for `text-generation`, and in the search bar if you type `gemma`, you will see many models (you can also try `qwen`). We will use the smallest model, `gemma-3-270m` (270 million parameters), so that we don't run out of memory.\n",
    "\n",
    "**In order to use `gemma` (like many other models), you need to:**\n",
    "1. **log into Huggingface;**\n",
    "2. **request access on the model page (it's granted automatically)!**\n",
    "\n",
    "It can be good to check larger versions and see if you notice a difference in the text quality. Without more advanced methods like freezing layers, or quantization + parameter-efficient methods, only the smallest one will fit in a free T4 Colab GPU!\n",
    "\n",
    "**Warning**: these models were trained on large portions of the Internet, and even despite a lot of work from corporation to make the models less biased/offensive, that still happens! In fact, many people, either users or researchers, try to find ways to remove the ethical guardrails – this is called ['jailbreaking'](https://www.ibm.com/think/insights/ai-jailbreak)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npCvBU8BBxVm"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"google/gemma-3-270m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer(['MERCUTIO:'] * 2, return_tensors='pt').to(device)\n",
    "\n",
    "# manually move the model to the GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(device)\n",
    "\n",
    "# our configuration\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_ID)\n",
    "generation_config.max_length = 100\n",
    "generation_config.do_sample = True\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.temperature = .9\n",
    "\n",
    "# generate text using the config\n",
    "output = model.generate(\n",
    "    **input_ids, generation_config=generation_config\n",
    ")\n",
    "\n",
    "# decode back from tokens to text\n",
    "texts = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "# print\n",
    "for t in texts:\n",
    "    wrap_print(t)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjWm-d9H9vta"
   },
   "source": [
    "#### Note on memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjWm-d9H9vta"
   },
   "source": [
    "If you try several models, before you do any training it is a good idea to clear the memory, using this:\n",
    "```python\n",
    "# del to delete any Python object\n",
    "del model\n",
    "# PyTorch to clear GPU mem (it can take a few moments to be executed!)\n",
    "torch.cuda.emtpy_cache()\n",
    "```\n",
    "\n",
    "However, sometimes it's just easier to restart the runtime ^^."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8ysxF7_El-n"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8ysxF7_El-n"
   },
   "source": [
    "## 1. Training in Python, with a Trainer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8ysxF7_El-n"
   },
   "source": [
    "Ported from the [Causal language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling) and [Fine-tune a language model](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb) tutorials. ([*So many other tutorials here, for all sorts of models*](https://huggingface.co/docs/transformers/notebooks)...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ycQ-jOVeURY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change if you need or work with another dataset!\n",
    "FINETUNED_MODEL_ID = \"gemma3-270m.tiny_shakespeare\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ycQ-jOVeURY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# updated from https://huggingface.co/datasets/karpathy/tiny_shakespeare\n",
    "dataset_raw = load_dataset(\n",
    "    \"jchwenger/tiny_shakespeare\",\n",
    "    # the dataset is three text files (train/validation/test), we need the\n",
    "    # following line to load them as entire chunks, instead of line by line\n",
    "    sample_by=\"document\"\n",
    ")\n",
    "\n",
    "print(dataset_raw)\n",
    "print(\"-\" *40)\n",
    "print(\"Number of characters per split:\")\n",
    "print([(split, len(dataset_raw[split][\"text\"][0])) for split in dataset_raw])\n",
    "print(\"-\" *40)\n",
    "print(dataset_raw[\"train\"][\"text\"][0][:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIrPM6XTewJU"
   },
   "source": [
    "See [here](https://huggingface.co/learn/nlp-course/chapter5/3#the-map-methods-superpowers) for an explanation of the `batched=True` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "uSdLl-ipLapj",
    "outputId": "d5b197ac-483d-4910-b91c-d75127aae1f8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "dataset_tok = dataset_raw.map(\n",
    "    tokenize_function,\n",
    "    # this 'batched' says the mapping will happen in parallel\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# the content of our \"text\" now lives in \"input_ids\", and since we only have one example\n",
    "# with all the text in it, we select it with [0] to see up to 40 tokens\n",
    "print(dataset_tok[\"train\"][\"input_ids\"][0][:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPKFVXmo2yt8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/5936c8c57ccb2bda3b3f28856a7ef992c5c9f451/examples/pytorch/language-modeling/run_clm.py#L516\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "\n",
    "# For older models, model.config.max_position_embeddings would work, but that number now is quite big\n",
    "# (This can be tweaked depending on the memory available – bigger number is good but needs more!)\n",
    "block_size = 1024\n",
    "\n",
    "def group_texts(ds):\n",
    "    # Concatenate all texts.\n",
    "    concat_ds = {k: sum(ds[k], []) for k in ds.keys()}\n",
    "    total_length = len(concat_ds[list(ds.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concat_ds.items()\n",
    "    }\n",
    "    # Important note: during training, the two sequences will be shifted by one,\n",
    "    # so that the model predicts the next step for each step. This is done auto-\n",
    "    # matically by Huggingface internally\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = dataset_tok.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "print(f\"Now our dataset contains {len(lm_dataset['train']['input_ids'])} examples,\")\n",
    "print(f\"each of length {len(lm_dataset['train']['input_ids'][0])} tokens (that we can feed in batches)...\")\n",
    "print(f\"(the 'block_size', aka 'attention window' of our model is {block_size=})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPsOz3uSvCob"
   },
   "source": [
    "### The Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlpKGOGbBW2P",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tune up or down depending on available memory\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=FINETUNED_MODEL_ID,\n",
    "    # small learning rate (bigger = faster training, but risks erasing prior memory)\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=5,\n",
    "    # only a few epochs, but that's something to tune (together with the learning rate)\n",
    "    num_train_epochs=4,\n",
    "    # you can also overrides num_train_epochs, to train even less than one epoch!\n",
    "    # max_steps=100,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    # can be \"no\", or \"steps\"/\"epoch\" (those two will include saving at the end)\n",
    "    # note that both strategies must be the same\n",
    "    eval_strategy=\"epoch\",    \n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    # uncomment to push to your HF account at the end of training\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset['train'],\n",
    "    eval_dataset=lm_dataset['validation'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzYkf6jmUBt1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPk7-VwB_Kpz"
   },
   "source": [
    "Models tend to suffer from \"catastrophic forgetting\" when you fine-tune, and it's often interesting to train for as little as possible to explore the change in the network output, before training a bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPk7-VwB_Kpz"
   },
   "source": [
    "#### Note on memory (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPk7-VwB_Kpz"
   },
   "source": [
    "When testing things and running into memory issues, as before one (imperfect) way to solve this is to delete the `trainer` variable and clear the GPU cache like so (otherwise, just restart the runtime and re-run only the cells you need):\n",
    "\n",
    "```python\n",
    "torch.cuda.empty_cache()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOA46EmIs3Z4"
   },
   "source": [
    "### Testing the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOA46EmIs3Z4"
   },
   "source": [
    "Here I go through the lower level steps of encoding text with our tokenizer, creating a batch of prefixes, defining a config, then generating using the model, instead of using a pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_u2RreSs3Z5"
   },
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer(['MERCUTIO:'] * 2, return_tensors='pt').to(device)\n",
    "\n",
    "# same logic as before\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_ID)\n",
    "generation_config.max_length = 100\n",
    "generation_config.do_sample = True\n",
    "generation_config.top_p = 0.98\n",
    "generation_config.temperature = .9\n",
    "\n",
    "# generate text using the config\n",
    "output = model.generate(\n",
    "    **input_ids,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "\n",
    "# decode back from tokens to text\n",
    "texts = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "# print\n",
    "for t in texts:\n",
    "    wrap_print(t)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY8QAVEgZmMC"
   },
   "source": [
    "### Saving your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY8QAVEgZmMC"
   },
   "source": [
    "If you want to save your model manually, you can just do:\n",
    "\n",
    "```python\n",
    "trainer.save_model(FINETUNED_MODEL_ID) # or some other directory\n",
    "```\n",
    "\n",
    "To save the model and tokenizer manually, you can do:\n",
    "\n",
    "```python\n",
    "tokenizer.save_pretrained(FINETUNED_MODEL_ID)\n",
    "model.save_pretrained(FINETUNED_MODEL_ID)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY8QAVEgZmMC"
   },
   "source": [
    "#### Note on Huggingface Hub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VY8QAVEgZmMC"
   },
   "source": [
    "[Share a model Huggingface tutorial](https://huggingface.co/docs/transformers/model_sharing)\n",
    "\n",
    " Huggingface `models`, `tokenizers`, and `trainers` all have a `.push_to_hub('my-model')` method, but the `trainer` will be the one saving everything you need.\n",
    "\n",
    " You can push your finetuned pipeline like so:\n",
    "\n",
    " ```python\n",
    " trainer.push_to_hub(FINETUNED_MODEL_ID) # or another name\n",
    " ```\n",
    "\n",
    "Once the model is on the hub, we can create a new pipeline you can now access your model from anywhere using `model='username/your-model-id'` (or any name you used for the output folder). You can also use the folder where you saved your model (`model=/path/to/your/model`).\n",
    "\n",
    "```python\n",
    "# replace with your own\n",
    "HF_USERNAME = \"jchwenger\"\n",
    "HF_HUB_MODEL_ID = f\"{HF_USERNAME}/{FINETUNED_MODEL_ID}\"\n",
    "\n",
    "generator = pipeline(\n",
    "    'text-generation',\n",
    "    # download the model from the hub\n",
    "    model=HF_HUB_MODEL_ID,\n",
    "    # this time, we need to specify which tokenizer to use\n",
    "    tokenizer=tokenizer,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "wrap_print(\n",
    "    generator(\n",
    "        \"MERCUTIO:\",\n",
    "        generation_config=generation_config,\n",
    "    )[0]['generated_text']\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy"
   },
   "source": [
    "### Search for other datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy"
   },
   "source": [
    "`tiny_shakespeare` is obviously not the only dataset availabe on the Hub. Another example I came across a few times recently is the [`english_quotes` dataset by Abirate](https://huggingface.co/datasets/Abirate/english_quotes). You can load it like so:\n",
    "\n",
    "```python\n",
    "dataset_raw = load_dataset(\"Abirate/english_quotes\")\n",
    "```\n",
    "\n",
    "The one thing to watch out for is that the text lives in `\"quote\"`, not in `\"text\"`! Your `tokenize_function`, for instance, should then work on `examples[\"quote\"]` rather than `examples[\"text\"]`, and so does the rest of this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy"
   },
   "source": [
    "### Freezing layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy"
   },
   "source": [
    "One strategy to save memory and to reduce the impact of fine-tuning is to finetune only *some* layers of the model (usually the top-ones, leaving the base 'frozen'). You can achieve that by:\n",
    "\n",
    "1. Looking at a list of all your layers:\n",
    "\n",
    "```python\n",
    "for i, (name, params) in enumerate(model.named_parameters()):\n",
    "    print(i, name)\n",
    "```\n",
    "2. Setting the `.requires_grad` attribute to `False` for most layers (here I just loop through them and freeze the first 122 layers (see [this link](https://discuss.huggingface.co/t/freeze-lower-layers-with-auto-classification-model/11386/2)):\n",
    "\n",
    "```python\n",
    "layer_threshold = 122\n",
    "for i, (name, params) in enumerate(model.named_parameters()):\n",
    "    if i < layer_threshold:\n",
    "        # print(f\"freezing: {name}\")\n",
    "        params.requires_grad = False\n",
    "```\n",
    "\n",
    "After doing this, you can notice that the memory footprint of the model is much less big on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy"
   },
   "source": [
    "### Working with raw text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "To load data from a file or a directory, see [this reference](https://huggingface.co/docs/datasets/nlp_load).\n",
    "\n",
    "There are various options available, either manually download the file, for instance like so:\n",
    "\n",
    "```python\n",
    "# let's download the tiny shakespeare dataset manually\n",
    "dataset_dir = \"text-dataset\"\n",
    "if not os.path.isdir(dataset_dir):\n",
    "    os.mkdir(dataset_dir)\n",
    "\n",
    "# move into the directory and download the file\n",
    "os.chdir(dataset_dir)\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "os.chdir('..')\n",
    "```\n",
    "\n",
    "Then we can load the dataset from the directory we created:\n",
    "\n",
    "```python\n",
    "dataset_raw_from_dir = load_dataset(\n",
    "    \"text\", # in this case we need \"text\" as a generic name to specify the task\n",
    "    data_dir=dataset_dir,\n",
    "    sample_by=\"document\"  # we indicate we want the whole text\n",
    ")                         # the default is line by line, \"paragraph\" cuts on empty lines\n",
    "# print(dataset_raw_from_dir[\"train\"][\"text\"][0][:250])\n",
    "print(len(dataset_raw_from_dir[\"train\"][\"text\"][0]))\n",
    "```\n",
    "\n",
    "Note that in the code above, `load_dataset` is usable if you have more than one files. You can also select which files go where like so: `data_files={\"train\": \"text-dataset/input.txt\"}`. Even more, you could skip the above step and just do `data_files=='https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'`.\n",
    "\n",
    "When we load raw files, the train/validation/test split is not done for us. *If you wanted to do this* (you could also not care about a validation dataset and just have \"train\", you wouldn't be the first person to do this), for this one file, you would do it this way:\n",
    "\n",
    "```python\n",
    "# adapted from ChatGPT-4 output\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def split_text_dataset(dataset, train_percent=0.9, validation_percent=0.05):\n",
    "    # Retrieve all texts\n",
    "    full_text = '\\n'.join(dataset[\"train\"][\"text\"])\n",
    "\n",
    "    # Calculate split lengths\n",
    "    total_length = len(full_text)\n",
    "    train_length = int(total_length * train_percent)\n",
    "    validation_length = int(total_length * validation_percent)\n",
    "\n",
    "    # Split the text\n",
    "    train_text = full_text[:train_length]\n",
    "    validation_text = full_text[train_length:train_length + validation_length]\n",
    "    test_text = full_text[train_length + validation_length:]\n",
    "\n",
    "    # Combine into a DatasetDict\n",
    "    return DatasetDict({\n",
    "        'train': Dataset.from_dict({'text': [train_text]}),\n",
    "        'validation': Dataset.from_dict({'text': [validation_text]}),\n",
    "        'test': Dataset.from_dict({'text': [test_text]})\n",
    "    })\n",
    "\n",
    "dataset_raw = split_text_dataset(dataset_raw_from_dir)\n",
    "```\n",
    "\n",
    "Now our dataset is almost the same as the one downloaded from the Hub:\n",
    "\n",
    "```python\n",
    "print(dataset_raw)\n",
    "print([(split, len(dataset_raw[split]['text'][0])) for split in dataset_raw])\n",
    "print(dataset_raw['train']['text'][0][:250])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper: Using `Supervised Fine-tuning` to turn a model into a chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the next notebook, and also [this](https://ai.google.dev/gemma/docs/core/huggingface_text_finetune_qlora).\n",
    "\n",
    "See also the full [NLP Course](https://huggingface.co/learn/nlp-course/chapter1/1) with videos and notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy"
   },
   "source": [
    "### Even deeper: the full language model pipeline from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48zRx_M5Cbgy"
   },
   "source": [
    "If you want to see how deep the rabbit hole goes, I can only recommend [this Colab](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=2c5V0FvqseE0) by the same Karpathy, accompanying [his tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY) (I highly recommend this series on building language models from scratch!).\n",
    "\n",
    "There is also this less low-level HF tutorial [\"Training a causal language model from scratch\n",
    "\"](https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt), and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section6_pt.ipynb)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
